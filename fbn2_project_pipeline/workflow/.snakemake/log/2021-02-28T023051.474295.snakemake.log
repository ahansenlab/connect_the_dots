Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	find_trajectory_pairs
	1	make_movies
	1	refine_tracks
	4

[Sun Feb 28 02:30:51 2021]
rule find_trajectory_pairs:
    input: results/tracks/2020_08_31_Fbn2_C36_DISH1_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie1-03_processed_low.czi.tracks.csv
    output: results/tracks/2020_08_31_Fbn2_C36_DISH1_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie1-03_processed_low.czi.joined_tracks.csv
    jobid: 5
    wildcards: sample=2020_08_31_Fbn2_C36_DISH1_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie1-03_processed_low.czi

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/hbrandao/libs/data_analysis_Fbn2/snakemake/workflow/.snakemake/log/2021-02-28T023051.474295.snakemake.log
