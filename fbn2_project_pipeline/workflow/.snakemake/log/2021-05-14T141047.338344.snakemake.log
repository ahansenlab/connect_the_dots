Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=240
Job counts:
	count	jobs
	1	all
	32	get_trajectories_from_timeseries
	33

[Fri May 14 14:10:47 2021]
rule get_trajectories_from_timeseries:
    input: /mnt/md0/Hansen Lab Dropbox/DataStorage/Imaging/Fbn2/processed/2021_05_C/2021_05_11_Fbn2_RAD21_twohoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie3-01.czi
    output: results/Rad21/2021_05_11_Fbn2_RAD21_twohoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie3-01.czi.tracks.csv, results/Rad21/2021_05_11_Fbn2_RAD21_twohoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie3-01.czi.dot_volumes.pkl
    jobid: 85
    wildcards: group=Rad21, sample=2021_05_11_Fbn2_RAD21_twohoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie3-01.czi
    resources: mem_gb=200

Terminating processes on user request, this might take some time.
Cancelling snakemake on user request.
