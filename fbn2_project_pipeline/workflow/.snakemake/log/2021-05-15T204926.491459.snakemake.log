Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=240
Job counts:
	count	jobs
	1	all
	22	get_trajectories_from_timeseries
	22	make_MIP_movies_from_trajectories
	45

[Sat May 15 20:49:27 2021]
rule get_trajectories_from_timeseries:
    input: /mnt/md0/Hansen Lab Dropbox/DataStorage/Imaging/Fbn2/processed/2020_12_A/2020_12_08_WAPL_sixhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01_processed_low.czi
    output: results/Rad21_6_hr/2020_12_08_WAPL_sixhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01_processed_low.czi.tracks.csv, results/Rad21_6_hr/2020_12_08_WAPL_sixhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01_processed_low.czi.dot_volumes.pkl
    jobid: 110
    wildcards: group=Rad21_6_hr, sample=2020_12_08_WAPL_sixhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01_processed_low.czi
    resources: mem_gb=200

Terminating processes on user request, this might take some time.
Job failed, going on with independent jobs.
Complete log: /home/hbrandao/libs/data_analysis_Fbn2/snakemake/workflow/.snakemake/log/2021-05-15T204926.491459.snakemake.log
