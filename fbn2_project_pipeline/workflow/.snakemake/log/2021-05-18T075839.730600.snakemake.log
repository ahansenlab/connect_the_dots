Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=240
Job counts:
	count	jobs
	1	all
	1	get_trajectories_from_timeseries
	2

[Tue May 18 07:58:40 2021]
rule get_trajectories_from_timeseries:
    input: /mnt/md0/Hansen Lab Dropbox/DataStorage/Imaging/Fbn2/processed/2021_05_D/2021_05_14_Fbn2_CTCF_fourhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01.czi
    output: results/CTCF_4_hr/2021_05_14_Fbn2_CTCF_fourhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01.czi.tracks.csv, results/CTCF_4_hr/2021_05_14_Fbn2_CTCF_fourhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01.czi.dot_volumes.pkl
    jobid: 258
    wildcards: group=CTCF_4_hr, sample=2021_05_14_Fbn2_CTCF_fourhoursIAA_488nm_0p5_561nm_0p05_SC8_2X_30z_250nm_780V_365T_20s_movie2-01.czi
    resources: mem_gb=200

Terminating processes on user request, this might take some time.
Job failed, going on with independent jobs.
Complete log: /home/hbrandao/libs/data_analysis_Fbn2/snakemake/workflow/.snakemake/log/2021-05-18T075839.730600.snakemake.log
