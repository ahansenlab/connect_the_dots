## This version 2021/03/02 uses an 'adaptive' fixed mask for localizing dots

# https://github.com/snakemake-workflows/cookiecutter-snakemake-workflow
# to first create the samples file
# snakemake -s prep_samples_snakefile --cores 1
# to run this workflow, use:
# snakemake -s snakefile_fixed_mask --resources mem_mb=240 --cores 1 --keep-going
# to make report: snakemake --report
# For reports need to install:
# sudo apt-get install graphviz graphviz-dev
# pip install pygraphviz
"""
Outline:
1. rule for beads
2. rule for loading and filtering 4D time-series
   --> rule for getting tracks from movies
3. rule for making movies from tracks
4. rule for chromatic shift correction
5. rule for running HMM on curated trajectories
6. 
"""

    
    
from snakemake.utils import validate
import pandas as pd
import os

configfile: "config/config.yaml"
validate(config, schema="schemas/config.schema.yaml")

samples_df = pd.read_csv(config["samples_file_name"], dtype = str)
validate(samples_df, schema="schemas/samples.schema.yaml")

SAMPLE_PATHS = samples_df.file_paths.values
SAMPLE_NAMES = samples_df.file_names.values
SAMPLE_GROUP = samples_df.group.values 

rule all:
    input:
        traj=expand("results/{group}/{sample}.tracks.csv",zip,sample=SAMPLE_NAMES,group=SAMPLE_GROUP),
        dots=expand("results/{group}/{sample}.dot_volumes.pkl",zip,sample=SAMPLE_NAMES,group=SAMPLE_GROUP),
        # mov=expand("results/{group}/{sample}.movie.mp4",zip,sample=SAMPLE_NAMES,group=SAMPLE_GROUP),
        # mov_filt=expand("results/{group}/{sample}.movie_filt.mp4",zip,sample=SAMPLE_NAMES,group=SAMPLE_GROUP)
     

def get_sample_path(wildcards):
    fname = wildcards.sample
    # assumes only one match
    fpath = samples_df[samples_df.file_names==fname].file_paths.values[0]    
    full_path = os.path.join(fpath,fname)
    return full_path   
    

rule get_trajectories_from_timeseries:
    resources : mem_gb = 200 # estimate for the amount of memory space used by this rule
    input:
        get_sample_path
    output:    
        traj="results/{group}/{sample}.tracks.csv",
        dots="results/{group}/{sample}.dot_volumes.pkl",
        #mov="results/{group}/{sample}.movie.mp4",
        #mov_filt="results/{group}/{sample}.movie_filt.mp4"
    run:
        # retrieve the raw data
        import numpy as np  
        import pandas as pd
        import pickle
        from connect_the_dots.io import get_CZI_metadata, get_CZI_zstack_timeseries
        from connect_the_dots.filtering import wavelet_filter_zstack        
        from connect_the_dots.tracking import get_localizations_iterative
        from connect_the_dots.tracking import filter_large_displacements_in_trackpy_trajectories
        from connect_the_dots.tracking import link_trajectories_across_channels
        from connect_the_dots.tracking import infer_missing_dot_locations
        from connect_the_dots.tracking import fill_missing_dot_localizations4
        from connect_the_dots.tracking import dot_volume_container      
        from connect_the_dots.io import make_movie
        
        print("Loading metadata...")
        
        data_info, metadata = get_CZI_metadata(input[0])
        frames = range(data_info[3])


        linked_df_dict = {}
        Z_dict = {}
        W_dict = {}
        for ch in [0,1]:
            
            # get the image time series
            print("Loading image time series...")
            Z = get_CZI_zstack_timeseries(input[0],frames,ch)
            Z_dict[ch] = Z
            
           
            # perform wavelet decomposition
            print("Performing wavelet decomposition...")
            W4 = wavelet_filter_zstack(Z,filtered_levels=[0,4])      
            W_dict[ch] = W4
            
            # localize dots
            print("Localizing dots, creating trajectories...")
            _ , linked_df = get_localizations_iterative(W4, Z, frames, ch,verbose=False)
            linked_df_dict[ch] = linked_df            
            
        # renumber the dot trajectory numbers
        particle_count_ch0 = len(linked_df_dict[0].particle.unique())
        for pi, p in enumerate(sorted(linked_df_dict[0].particle.unique())):
            linked_df_dict[0]['particle'].replace(p,pi,inplace=True)
        for pi, p in enumerate(sorted(linked_df_dict[1].particle.unique())):
            linked_df_dict[1]['particle'].replace(p,pi+particle_count_ch0,inplace=True)

        # quality check the trajectories (ensure there are no huge jumps)
        print("Quality checking trajectories...")
        linked_df = pd.concat([linked_df_dict[0],linked_df_dict[1]])
        linked_df = filter_large_displacements_in_trackpy_trajectories(linked_df,
                                                                       distance_max=35)
        
        # link trajectories across channels
        print("Linking trajectories...")
        joined_df, good_ids = link_trajectories_across_channels(linked_df,
                                                                min_overlap_length=20,
                                                                corrcoeff_min=0.5)
        # keep only the "good" trajectories
        print("Keeping only good trajectories...")
        joined_df = joined_df[joined_df.particle.apply(lambda x: x in good_ids)]
        #joined_df.to_csv(output.tracks)        
        

        # perform "gap filling inference" - estimate the missing "in between" dot positions
        print("Inferring trajectory gaps...")
        search_df = infer_missing_dot_locations(joined_df, max_gap_length=10)
        
        # actually "fill in" missing data points in space and time and refine the localizations
        print("Gap filling and extension...")
        
        filled_df, dot_volumes = fill_missing_dot_localizations4(Z_dict,
                                   search_df,
                                   window_XYZ=(6,6,4),
                                   radius_xy=4,
                                   radius_z=3,
                                   verbose=False)

        # save trajectory files
        print(f"Saving trajectory... {output.traj}")
        filled_df.to_csv(output.traj)
               
        # save the dot "crops"
        print(f"Saving dot volumes... {output.dots}")
        pickle.dump(dot_volumes,open(output.dots,'wb'))
    
        """
        # make movies
        print(f"Making raw movies and trajectories")
        make_movie(Z_dict,
               filled_df,
               output_filename=output.mov,
               metadata=metadata,
               desired_bar_length_um=10,
               percent_cutoff=99.99,
               disp_line_len=25,
               verbose=False,
               max_axis=0,
               text_dist=10,
               adaptive_text=False,
               line_alpha=0.5,
               adaptive_LUT=True,
               millisecond_per_frame=100)

        print(f"Making filtered movies and trajectories")
        make_movie(W_dict,
               filled_df,
               output_filename=output.mov_filt,
               metadata=metadata,
               desired_bar_length_um=10,
               percent_cutoff=99.99,
               disp_line_len=25,
               verbose=True,
               max_axis=0,
               text_dist=10,
               adaptive_text=False,
               line_alpha=0.5,
               adaptive_LUT=True,
               millisecond_per_frame=100)   
        
        print(f"Done! {output.mov_filt}")
        """
        
rule make_MIP_movies_from_trajectories:
    input:
        get_sample_path, 
        "results/{group}/{sample}.tracks.csv",
    output:    
        mov="results/{group}/{sample}.movie.mp4",
    run:  
        # retrieve the raw data
        import numpy as np  
        import pandas as pd
        import pickle
        from connect_the_dots.io import get_CZI_metadata, get_CZI_zstack_timeseries 
        from connect_the_dots.io import make_movie
        import os
        
        #print(input[1])
        #print(input[0])
        #print(output.mov)
        
        # Load the trajectories
        print("Getting trajectories...") 
        filled_df = pd.read_csv(input[1])
        
        print("Loading metadata...")       
        data_info, metadata = get_CZI_metadata(input[0])
        frames = range(data_info[3])        
        
        Z_dict = {}
        for ch in [0,1]:
            # get the image time series
            print("Loading image time series...")
            Z = get_CZI_zstack_timeseries(input[0],frames,ch)
            Z_dict[ch] = Z

        
        # make movies
        print(f"Making movies with trajectories")
        make_movie(Z_dict,
               filled_df,
               output_filename=output.mov,
               metadata=metadata,
               desired_bar_length_um=10,
               percent_cutoff=99.99,
               disp_line_len=25,
               verbose=False,
               max_axis=0,
               text_dist=10,
               adaptive_text=False,
               line_alpha=0.5,
               adaptive_LUT=True,
               millisecond_per_frame=100)
               
        
              
        
               
               
               
               
               



