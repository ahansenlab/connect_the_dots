from snakemake.utils import validate
import pandas as pd
import os

configfile: "config/config.yaml"
validate(config, schema="schemas/config.schema.yaml")

# BEADS
beads_df = pd.read_csv(config["beads_file_name"], dtype = str)
validate(beads_df, schema="schemas/samples.schema.yaml")
BEAD_PATHS = beads_df.file_paths.values
BEAD_NAMES = beads_df.file_names.values
BEAD_GROUP = beads_df.group.values

# SAMPLES
samples_df = pd.read_csv(config["samples_file_name"], dtype = str)
validate(samples_df, schema="schemas/samples.schema.yaml")
SAMPLE_PATHS = samples_df.file_paths.values
SAMPLE_NAMES = samples_df.file_names.values
SAMPLE_GROUP = samples_df.group.values


"""
# EXISTING SAMPLES (TEMPORARY)
#from connect_the_dots.io import search_for
from pathlib import Path
def search_for(start_folder, name_includes=None, name_excludes=None):
    filenames_list = []
    filepath_list = []
    for path in Path(start_folder).rglob('*.*'):

        parent_folder = path.parent.name
        if parent_folder in name_excludes:
            continue   

        if (all([name.lower() in path.name.lower() for name in name_includes])==False) or \
            any([name.lower() in path.name.lower() for name in name_excludes])==True:
            continue
            
        filenames_list.append(path.name)
        filepath_list.append(os.path.join(start_folder,path.parent.name))
    return filenames_list, filepath_list
existing_names, existing_folders = search_for('/mnt/md0/Hugo/',name_includes=['.tracks.csv'],name_excludes=['sdfsadfasdf'])
idx = [i for i, n in enumerate(existing_folders) if 'tra' not in n]
existing_folders = [existing_folders[i] for i in idx]
existing_names = [existing_names[i].split('.track')[0] for i in idx]
existing_group = [f.split('/')[-1] for f in existing_folders]
SAMPLE_PATHS = existing_folders 
SAMPLE_NAMES = existing_names 
SAMPLE_GROUP = existing_group 
"""


# HELPER FUNCTIONS
def get_sample_path(wildcards):
    fname = wildcards.sample_name
    # assumes only one match
    fpath = samples_df[samples_df.file_names==fname].file_paths.values[0]    
    full_path = os.path.join(fpath,fname)
    return full_path  
    
def get_sample_names_from_group(wildcards):
    group = wildcards.sample_group
    # get all matches
    fpaths = samples_df[samples_df.group==group].file_paths.values   
    fnames = samples_df[samples_df.group==group].file_paths.values   
    full_paths = []
    for fp, fn in zip(fpaths,fnames):
        full_paths.append(os.path.join(fp,fn))
    return full_paths      
        
def get_bead_path(wildcards):
    fname = wildcards.bead_name
    # assumes only one match
    fpath = beads_df[beads_df.file_names==fname].file_paths.values[0]    
    full_path = os.path.join(fpath,fname)
    return full_path      
    

# RULES
rule all:
    input:
        beads = expand("results/{bead_group}/{bead_name}.localizations.csv", 
                        zip,bead_group=BEAD_GROUP,bead_name=BEAD_NAMES),
        #chromatic_shift = expand("results/{sample_group}/{sample_name}.chromatic_shift.npz",
        #                zip,sample_group=SAMPLE_GROUP,sample_name=SAMPLE_NAMES),
        #traj=expand("results/{sample_group}/{sample_name}.tracks_corrected.csv",
        #                zip,sample_group=SAMPLE_GROUP,sample_name=SAMPLE_NAMES),
        #traj_ml=expand("results/{sample_group}/{sample_name}.tracks_ML.csv",
        #                zip,sample_group=SAMPLE_GROUP,sample_name=SAMPLE_NAMES),
        summary_sheets = expand("results/z_summaries/Summary_stats_{sample_group}.csv",
                        sample_group=SAMPLE_GROUP)

rule get_bead_localizations:
    resources : mem_gb = 50 # estimate for the amount of memory space used by this rule
    input:
        get_bead_path
    output:    
        localizations="results/{bead_group}/{bead_name}.localizations.csv"
    run:
        import numpy as np  
        import pandas as pd
        from connect_the_dots.io import get_CZI_metadata, get_CZI_zstack_timeseries    
        from connect_the_dots.tracking import get_localizations_iterative
                
        print("Loading metadata...")
        data_info, metadata = get_CZI_metadata(input[0])
        frames = range(data_info[3])

        linked_df_dict = {}
        for ch in [0,1]:
            
            # get the image time series
            print("Loading image time series...")
            Z = get_CZI_zstack_timeseries(input[0],frames,ch)
            
            # localize dots
            print("Localizing dots, creating trajectories...")
            _ , linked_df = get_localizations_iterative(Z, Z, frames, ch,
                                                        verbose=False,
                                                        min_track_length=4,
                                                        percentile_threshold=99.95, # 99.9
                                                        max_dot_size = 20000,
                                                        min_dot_size = 5,
                                                        percentile_threshold_increment = 0.1,
                                                        search_range=(3,5,5))
            linked_df_dict[ch] = linked_df  
        

        # renumber the dot trajectory numbers
        particle_count_ch0 = len(linked_df_dict[0].particle.unique())
        for pi, p in enumerate(sorted(linked_df_dict[0].particle.unique())):
            linked_df_dict[0]['particle'].replace(p,pi,inplace=True)
        for pi, p in enumerate(sorted(linked_df_dict[1].particle.unique())):
            linked_df_dict[1]['particle'].replace(p,pi+particle_count_ch0,inplace=True)
    
        linked_df = pd.concat([linked_df_dict[0],linked_df_dict[1]])
        linked_df.to_csv(output.localizations)        


rule get_chromatic_shift_corrections:
    input:        
        bead_localizations=expand("results/{bead_group}/{bead_name}.localizations.csv", 
                        zip,bead_group=BEAD_GROUP,bead_name=BEAD_NAMES),       
        traj="results/{sample_group}/{sample_name}.tracks.csv",
        sample_czi=get_sample_path,
    output:    
        chromatic_shift="results/{sample_group}/{sample_name}.chromatic_shift.npz"
    script:
        "scripts/get_chromatic_shifts.py"


rule apply_chromatic_shift_corrections:
    input:
        chromatic_shift = "results/{sample_group}/{sample_name}.chromatic_shift.npz",
        traj = "results/{sample_group}/{sample_name}.tracks.csv",   
    output:
        traj = "results/{sample_group}/{sample_name}.tracks_corrected.csv",
    script: 
        "scripts/apply_chromatic_shifts.py"
        


rule classify_trajectories:
    input:
        traj="results/{sample_group}/{sample_name}.tracks_corrected.csv",
        dot_volumes="results/{sample_group}/{sample_name}.dot_volumes.pkl",
        ml_replication_model = config["ml_replication_model"],
        ml_good_bad_model = config["ml_good_bad_model"],             
    output:
        traj_ml="results/{sample_group}/{sample_name}.tracks_ML.csv",
    script:
        "scripts/classify_dots_by_CNN.py"




rule compute_summary_statistics:
    input:
        traj_ml = expand("results/{sample_group}/{sample_name}.tracks_ML.csv",
                        zip,sample_group=SAMPLE_GROUP,sample_name=SAMPLE_NAMES),
    output:
        summary_sheets="results/z_summaries/Summary_stats_{sample_group}.csv",
    script:
        "scripts/create_summary_statistics.py"      
            
        
"""
rule convert_into_tracklib_format:
    input:
        traj="results/{sample_group}/{sample_name}.tracks_ML.csv",
    output:
        traj="results/{sample_group}/{sample_name}.tagged_set.csv",
    script:
        ""
        
    
rule compute_histograms:
    input:
        traj=expand("results/{sample_group}/{sample_name}.tagged_set.csv",
                        sample_group=SAMPLE_GROUP,sample_name=SAMPLE_NAME)),
    output:
        figures="results/Figures/{sample_group}/Histogram_3D_distances_{sample_group}.pdf",
    script:
        ""

rule apply_inference:
    input:
        traj=expand("results/{sample_group}/{sample_name}.tagged_set.csv",
                        sample_group=SAMPLE_GROUP,sample_name=SAMPLE_NAME)),
    output:
        figures="ML.pdf"
    script:
        ""
"""    
